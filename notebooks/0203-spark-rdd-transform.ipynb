{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515de390",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d87276",
   "metadata": {},
   "source": [
    "# RDD Transform Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c28fc",
   "metadata": {},
   "source": [
    "## map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8336fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_square = rdd.map(lambda x: x**2)\n",
    "rdd_cube = rdd.map(lambda x: x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a754ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Square: \" + repr(rdd_square.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c20c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cube: \" + repr(rdd_cube.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d88ea4",
   "metadata": {},
   "source": [
    "Nothing changes in `rdd`. It generate a new RDD when calling transform methods on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original: \" + repr(rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae26fb9",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00be1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_even = rdd.filter(lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Even: \" + repr(rdd_even.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22148090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def is_prime(n):\n",
    "    for i in range(2, int(math.ceil(math.sqrt(n))) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "rdd_prime = rdd.filter(is_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prime: \" + repr(rdd_prime.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437592d",
   "metadata": {},
   "source": [
    "## flatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f1c84",
   "metadata": {},
   "source": [
    "Flatten a result which is a list of collection of items as a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_cube(n):\n",
    "    return n, n**2, n**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map = rdd.map(power_cube)\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63885137",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_flatten_map = rdd.flatMap(power_cube)\n",
    "rdd_flatten_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76388e4",
   "metadata": {},
   "source": [
    "## mapPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89b6fa",
   "metadata": {},
   "source": [
    "Instead to process values in `rdd` one time for a function call, this function supports to process one function call for a collection of values in `rdd` partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(partition_data):\n",
    "    for x in partition_data:\n",
    "        yield x**2\n",
    "        \n",
    "rdd.mapPartitions(process_partitions).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_five_partitions = rdd.repartition(5)\n",
    "rdd_five_partitions.mapPartitions(process_partitions).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8f413",
   "metadata": {},
   "source": [
    "## reduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a329c3",
   "metadata": {},
   "source": [
    "First of all, we must make an RDD of key-value pair. It is simple to do by using `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "    0: \"even\",\n",
    "    1: \"odd\", \n",
    "}\n",
    "\n",
    "rdd_kv = rdd.map(lambda x: (keys[x % 2], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34996f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_kv.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93491947",
   "metadata": {},
   "source": [
    "## groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ef112",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd_kv = rdd_kv.groupByKey()\n",
    "grouped_rdd_kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5827ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd_kv.mapValues(len).collect()    # grouped_rdd_kv.map(lambda x: (x[0], len(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935dfe4",
   "metadata": {},
   "source": [
    "## combineByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475be9e0",
   "metadata": {},
   "source": [
    "In some cases that need to control aggregation in fine grain. `combineByKey` provide that capability for us. Actually this is not neccessary for learning, but just demonstrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b98a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_kv_2 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 3), (\"c\", 1), (\"b\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261761ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(n):\n",
    "    return [n]\n",
    "\n",
    "def append(p, n):\n",
    "    p.append(n)\n",
    "    return p\n",
    "\n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "rdd_kv_2.combineByKey(\n",
    "    createCombiner=to_list,\n",
    "    mergeValue=append,\n",
    "    mergeCombiners=extend\n",
    ").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
