{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cd4e7",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1efa6",
   "metadata": {},
   "source": [
    "## How to create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d035fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778a6e7",
   "metadata": {},
   "source": [
    "#### createDataFrame and toDF\n",
    "`createDataFrame()` is a primary method in `SparkSession` to create a dataframe from other data. Another one is `toDF()` in a RDD object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f737406",
   "metadata": {},
   "source": [
    "### Using list of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [\n",
    "    (\"Finance\",10), \n",
    "    (\"Marketing\",20), \n",
    "    (\"Sales\",30), \n",
    "    (\"IT\",40), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07407bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbec24f",
   "metadata": {},
   "source": [
    "#### printSchema\n",
    "`printSchema()` is used to show column schemas of a dataframe in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23485d",
   "metadata": {},
   "source": [
    "#### show\n",
    "`show()` is used to display dataframe data in table text format. There are two parameters for the function `n`, the number of data to display with default as 50 and `truncate` which is a boolean or number to indicate how width allow before trimming the column output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3177872",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5d4eb",
   "metadata": {},
   "source": [
    "To specify schema without letting Spark infers from data, we will create `StructType` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33240374",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptSchema = T.StructType([       \n",
    "    T.StructField(\"dept_name\", T.StringType(), True),\n",
    "    T.StructField(\"dept_id\", T.LongType(), True),\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema=deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabd1b9",
   "metadata": {},
   "source": [
    "### Using list of Row (Spark data type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept2 = [\n",
    "    T.Row(\"Finance\",10), \n",
    "    T.Row(\"Marketing\",20), \n",
    "    T.Row(\"Sales\",30), \n",
    "    T.Row(\"IT\",40) \n",
    "]\n",
    "\n",
    "deptDF2 = spark.createDataFrame(data=dept, schema = deptColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa296e1f",
   "metadata": {},
   "source": [
    "#### collect\n",
    "`show()` does not return data in dataframe to **driver**. As a RDD has `collect()` to retrieve all data into **driver**, a dataframe also has one. However the type of item returned is `Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a697d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02342194",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c6263",
   "metadata": {},
   "source": [
    "## DataFrame and RDD relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1be6d8",
   "metadata": {},
   "source": [
    "### Without schema and colum names\n",
    "\n",
    "Convert a RDD into DataFrame without column name and schema. Spark will infer schema and assign default column name `_n` (n is order sequence of a column, starting from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fe4fa",
   "metadata": {},
   "source": [
    "### With column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e467f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fabb3",
   "metadata": {},
   "source": [
    "Another way to pass column names in `toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fae01",
   "metadata": {},
   "source": [
    "Otherwise `Row` type can be used as data collection with the colum name arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1caf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowData = map(lambda x: T.Row(*x), data)\n",
    "\n",
    "dfFromData3 = spark.createDataFrame(rowData,columns)\n",
    "dfFromData3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f03de5",
   "metadata": {},
   "source": [
    "## Empty DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74386db0",
   "metadata": {},
   "source": [
    "As an empty data has no data, Spark cannot infer schema from the data. You must pass schema argument for `createDataFrame()` and `toDF()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e11753",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"firstName\", T.StringType(), True),\n",
    "    T.StructField(\"middleName\", T.StringType(), True),\n",
    "    T.StructField(\"lastName\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "emptyDF1 = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "emptyDF1.printSchema()\n",
    "print(\"Total rows: \" + str(emptyDF1.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e711db",
   "metadata": {},
   "source": [
    "Alternatives to create an empty dataframe with the same scheam as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66282d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyDF2 = spark.sparkContext.parallelize([]).toDF(schema)\n",
    "emptyDF3 = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369e374",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"city\", T.StringType(), True),\n",
    "    T.StructField(\"dates\", T.StringType(), True),\n",
    "    T.StructField(\"population\", T.IntegerType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"1991-02-25\",\"1998-05-10\", \"1993/03/15\", \"1992/07/17\"]\n",
    "cities = ['Caracas', 'Ccs', '   SÃ£o Paulo   ', '~Madrid']\n",
    "population = [37800000, 19795791, 12341418, 6489162]\n",
    "\n",
    "data = list(zip(cities, dates, population))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame with schema\n",
    "df = spark.createDataFrame(list(zip(cities, dates, population)), schema=schema)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894c439",
   "metadata": {},
   "source": [
    "## DataFrame Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26064d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Scott', 50), ('Jeff', 45), ('Thomas', 54), ('Ann', 34)]\n",
    "sparkDF = spark.createDataFrame(data, [\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str((sparkDF.count(), len(sparkDF.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c10ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparkShape(dataFrame):\n",
    "    return (dataFrame.count(), len(dataFrame.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6156d",
   "metadata": {},
   "source": [
    "Exending Spark DataFrame methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.sql.dataframe.DataFrame.shape = sparkShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b09ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sparkDF.shape())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
