{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c320aa",
   "metadata": {},
   "source": [
    "# Spark SQL as a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d580a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ab1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000),\n",
    "]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eec08a",
   "metadata": {},
   "source": [
    "## `spark.sql()`  to execute Spark SQL statement\n",
    "\n",
    "Spark provides `sql()`in `SparkSession` to allow us to use SQL statement with data frame, however without register a dataframe as a temporary view, Spark will not know what is the table name to reference (and a table name is a part of a `catalog` system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536104a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea635ffd",
   "metadata": {},
   "source": [
    "## `createOrReplaceTempView()` to register a dataframe as a tempoary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e199f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"EMPLOYEES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106cbe3",
   "metadata": {},
   "source": [
    "After registering the `df` dataframe as temporary called `EMPLOYEES`, we can now call sql command to work with data in dataframe like DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from EMPLOYEES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bfaba",
   "metadata": {},
   "source": [
    "As the output has shown, the result of `spark.sql()` is a dataframe which can be displayed by calling `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dcf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from EMPLOYEES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc75fdf",
   "metadata": {},
   "source": [
    "In this learning environment, the `sparkmagic` kernel provide us with `%%sql` to allow us execute SQL command directly without passing as an argument in `spark.sql()`. Additionally the kernel displays the return result automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select * from EMPLOYEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625afe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from EMPLOYEES\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f372029",
   "metadata": {},
   "source": [
    "## Default Spark Catalog  (SparkSessionCatalog)\n",
    "\n",
    "Actually when a SparkSession starts, it will create a empty Spark catalog system. Normally a catalog will contains metadata related with database, table, view, and temporary view (which exists within a session). So our `df` dataframe which is registered as `EMPLOYEES` will be terminated after the session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d33425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please restart kernel and execute the first cell to trigger creating a newly SparkSession\n",
    "# After that try to execute following command\n",
    "spark.sql(\"select * from EMPLOYEES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9bb721",
   "metadata": {},
   "source": [
    "## How to manage with a Spark catalog\n",
    "\n",
    "Spark provides a catalog object for working with metadata about databases, tables, views, and temporary views. We can access it view `spark.catalog` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1501e13",
   "metadata": {},
   "source": [
    "### List all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0e686",
   "metadata": {},
   "source": [
    "### List all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables(\"default\")  # or spark.catalog.listTables() because \"default\" is a default argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fe23c",
   "metadata": {},
   "source": [
    "### Drop a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b820703",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"EMPLOYEES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a44838",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from EMPLOYEES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae58ab",
   "metadata": {},
   "source": [
    "There are more to deep dive at [PySpark Catalog API](https://spark.apache.org/docs/3.2.4/api/python/reference/api/pyspark.sql.Catalog.html?highlight=catalog#pyspark.sql.Catalog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
